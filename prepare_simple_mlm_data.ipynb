{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random.seed(93)\n",
    "np.random.seed(93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "['input_ids', 'token_type_ids', 'attention_mask']"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "list(bert_tokenizer([\"The quick brown fox\", \"jumped over the lazy dog\"]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def sliding_window(lst: List[Any], max_len: int, overlaps: int):\n",
    "    return [lst[i:i + max_len] for i in range(0, max_len, overlaps)]\n",
    "\n",
    "def chunk_data(batch: List[Dict[str, Any]], max_seq=512, overlaps=256):\n",
    "    tokenized_texts = bert_tokenizer(batch[\"text\"])\n",
    "    effective_len = max_seq - 2 # [CLS] and [SEP]\n",
    "    result = {}\n",
    "    for k in tokenized_texts:\n",
    "        result[k] = []\n",
    "        for items in tokenized_texts[k]:\n",
    "            result[k].extend([*sliding_window(items, effective_len, overlaps)])\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['id', 'url', 'title', 'text'],\n        num_rows: 187451\n    })\n    valid: Dataset({\n        features: ['id', 'url', 'title', 'text'],\n        num_rows: 20828\n    })\n})"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikisource_ds = load_dataset(\"data\", data_files={\n",
    "    \"train\": \"wikisource_train.parquet\",\n",
    "    \"valid\": \"wikisource_valid.parquet\"\n",
    "})\n",
    "wikisource_ds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/187451 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ec9f3f51311e42bfbd87363886e57a5d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3314 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/20828 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5a272a34a206428a81c34f695d566c8e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 374902\n    })\n    valid: Dataset({\n        features: ['input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 41656\n    })\n})"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikisource_ds = wikisource_ds.map(chunk_data, batched=True, remove_columns=wikisource_ds[\"train\"].column_names)\n",
    "wikisource_ds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "Creating parquet from Arrow format:   0%|          | 0/375 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "94c765188ca54fd8bbb64624b03d84b4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Creating parquet from Arrow format:   0%|          | 0/42 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "af01d8b13176459b90ccef6ea783f566"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wikisource_ds[\"train\"].to_parquet(\"data/wikisource_train_512.parquet\")\n",
    "wikisource_ds[\"valid\"] = wikisource_ds[\"valid\"].to_parquet(\"data/wikisource_eval_512.parquet\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
