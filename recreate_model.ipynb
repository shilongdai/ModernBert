{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-20 20:46:11.811805: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-20 20:46:11.811836: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-20 20:46:11.812763: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-20 20:46:11.817364: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-20 20:46:12.370985: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from torch.nn import ModuleList\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertForMaskedLM\n",
    "from transformers.models.bert.modeling_bert import BertEmbeddings, BertModel\n",
    "\n",
    "random.seed(93)\n",
    "np.random.seed(93)\n",
    "torch.manual_seed(93)\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Recreating BERT Github in PyTorch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'[CLS] Mothers give [MASK] to children. [SEP]'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "masked = 4\n",
    "example_str = \"Mothers give birth to children.\"\n",
    "exampled_tokenized = tokenizer(example_str)\n",
    "exampled_tokenized[\"input_ids\"][masked] = tokenizer.mask_token_id\n",
    "tokenizer.decode(exampled_tokenized[\"input_ids\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': [101, 4872, 1116, 1660, 103, 1106, 1482, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exampled_tokenized"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "[('bert/embeddings/LayerNorm/beta', [768]),\n ('bert/embeddings/LayerNorm/gamma', [768]),\n ('bert/embeddings/position_embeddings', [512, 768]),\n ('bert/embeddings/token_type_embeddings', [2, 768]),\n ('bert/embeddings/word_embeddings', [28996, 768]),\n ('bert/encoder/layer_0/attention/output/LayerNorm/beta', [768]),\n ('bert/encoder/layer_0/attention/output/LayerNorm/gamma', [768]),\n ('bert/encoder/layer_0/attention/output/dense/bias', [768]),\n ('bert/encoder/layer_0/attention/output/dense/kernel', [768, 768]),\n ('bert/encoder/layer_0/attention/self/key/bias', [768]),\n ('bert/encoder/layer_0/attention/self/key/kernel', [768, 768]),\n ('bert/encoder/layer_0/attention/self/query/bias', [768]),\n ('bert/encoder/layer_0/attention/self/query/kernel', [768, 768]),\n ('bert/encoder/layer_0/attention/self/value/bias', [768]),\n ('bert/encoder/layer_0/attention/self/value/kernel', [768, 768]),\n ('bert/encoder/layer_0/intermediate/dense/bias', [3072]),\n ('bert/encoder/layer_0/intermediate/dense/kernel', [768, 3072]),\n ('bert/encoder/layer_0/output/LayerNorm/beta', [768]),\n ('bert/encoder/layer_0/output/LayerNorm/gamma', [768]),\n ('bert/encoder/layer_0/output/dense/bias', [768]),\n ('bert/encoder/layer_0/output/dense/kernel', [3072, 768])]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_chkpt = \"./models/cased_L-12_H-768_A-12/bert_model.ckpt\"\n",
    "tf.train.list_variables(tf_chkpt)[:21]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class BertConfig:\n",
    "\n",
    "    def __init__(self, vocab_size: int, vocab_pad: int = 0, d_model: int = 768, inter_size: int = 3072,\n",
    "                 inter_activation: str = \"GELU\", seq_len: int = 512, attention_heads=12,\n",
    "                 encoder_layers=12, layer_norm_eps=1e-12, hidden_dropout=0.1,\n",
    "                 attn_dropout=0.1):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab_pad = vocab_pad\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.attention_head = attention_heads\n",
    "        self.hidden_dropout = hidden_dropout\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.inter_size = inter_size\n",
    "        self.inter_activation = inter_activation\n",
    "        self.encoder_layers = encoder_layers\n",
    "\n",
    "\n",
    "class BertEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, config: BertConfig):\n",
    "        super().__init__()\n",
    "        self.word_embedding = nn.Embedding(num_embeddings=config.vocab_size, embedding_dim=config.d_model,\n",
    "                                           padding_idx=config.vocab_pad)\n",
    "        self.segment_embedding = nn.Embedding(num_embeddings=2, embedding_dim=config.d_model)\n",
    "        self.pos_embedding = nn.Embedding(num_embeddings=config.seq_len, embedding_dim=config.d_model)\n",
    "        self.layer_norm = nn.LayerNorm(config.d_model, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(p=config.hidden_dropout)\n",
    "\n",
    "    def forward(self, seq, seq_seg):\n",
    "        seq_positions = torch.IntTensor(list(range(seq.shape[1])))\n",
    "        embedding = self.word_embedding(seq) + self.pos_embedding(seq_positions) + self.segment_embedding(seq_seg)\n",
    "        return self.dropout(self.layer_norm(embedding))\n",
    "\n",
    "\n",
    "def load_tf_var(chpt: str, src_var: str, target: nn.parameter.Parameter, processor=lambda x: x):\n",
    "    src_val = tf.train.load_variable(chpt, src_var)\n",
    "    src_val = processor(src_val)\n",
    "    src_val = torch.from_numpy(src_val).float()\n",
    "    target.copy_(src_val)\n",
    "    assert torch.sum(target - src_val) <= 0.0001\n",
    "\n",
    "\n",
    "def load_embeddings(tf_chk, embedding: BertEmbedding):\n",
    "    with torch.no_grad():\n",
    "        # layer norm\n",
    "        load_tf_var(tf_chk, \"bert/embeddings/LayerNorm/gamma\", embedding.layer_norm.weight)\n",
    "        load_tf_var(tf_chk, \"bert/embeddings/LayerNorm/beta\", embedding.layer_norm.bias)\n",
    "\n",
    "        # token embeddings\n",
    "        load_tf_var(tf_chk, \"bert/embeddings/word_embeddings\", embedding.word_embedding.weight)\n",
    "        load_tf_var(tf_chk, \"bert/embeddings/position_embeddings\", embedding.pos_embedding.weight)\n",
    "        load_tf_var(tf_chk, \"bert/embeddings/token_type_embeddings\", embedding.segment_embedding.weight)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.4496,  0.0977, -0.2074,  ...,  0.0578,  0.0406, -0.0951],\n        [ 0.0540,  0.3217,  0.6037,  ...,  0.3489, -0.8150, -0.2603],\n        [ 0.4450,  0.7442,  0.6840,  ...,  0.5870,  0.7651, -0.7093],\n        ...,\n        [ 0.8370, -0.7599,  0.0051,  ..., -0.2732, -0.8569, -0.5360],\n        [-0.4275,  0.6968,  0.8957,  ...,  0.1489,  0.6208,  0.2714],\n        [-0.3162,  0.1007,  0.1413,  ...,  0.5393, -0.4997,  0.3309]])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = BertConfig(vocab_size=28996)\n",
    "embedding = BertEmbedding(config)\n",
    "load_embeddings(tf_chkpt, embedding)\n",
    "embedding.eval()\n",
    "with torch.no_grad():\n",
    "    example = embedding(torch.IntTensor([exampled_tokenized[\"input_ids\"]]),\n",
    "                        seq_seg=torch.IntTensor([exampled_tokenized[\"token_type_ids\"]]))\n",
    "example[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config: BertConfig):\n",
    "        super().__init__()\n",
    "        assert config.d_model % config.attention_head == 0\n",
    "\n",
    "        self.heads = config.attention_head\n",
    "        self.size_per_head = config.d_model // self.heads\n",
    "        self.d_model = config.d_model\n",
    "\n",
    "        self.q_proj = nn.Linear(in_features=self.d_model, out_features=self.d_model)\n",
    "        self.k_proj = nn.Linear(in_features=self.d_model, out_features=self.d_model)\n",
    "        self.v_proj = nn.Linear(in_features=self.d_model, out_features=self.d_model)\n",
    "        self.o_proj = nn.Linear(in_features=self.size_per_head * self.heads, out_features=self.d_model)\n",
    "        self.attn_dropout = nn.Dropout(config.attn_dropout)\n",
    "        self.output_dropout = nn.Dropout(config.hidden_dropout)\n",
    "        self.output_norm = nn.LayerNorm(self.d_model, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, seq: torch.Tensor, mask: torch.Tensor, output_attentions=False):\n",
    "        k_proj = self.multihead_view(self.k_proj(seq), transpose=True)\n",
    "        q_proj = self.multihead_view(self.q_proj(seq))\n",
    "        v_proj = self.multihead_view(self.v_proj(seq))\n",
    "\n",
    "        raw_scores = torch.matmul(q_proj, k_proj) / np.sqrt(q_proj.shape[-1])\n",
    "        masked_scores = raw_scores.masked_fill(mask == 0, -10000)\n",
    "        scaled_scores = torch.nn.functional.softmax(masked_scores, dim=-1)\n",
    "        scaled_scores = self.attn_dropout(scaled_scores)\n",
    "        results = torch.matmul(scaled_scores, v_proj).permute(0, 2, 1, 3)\n",
    "        combined = results.reshape(seq.shape[0], seq.shape[1], -1)\n",
    "\n",
    "        o_proj = self.output_dropout(self.o_proj(combined))\n",
    "        new_embedding = self.output_norm(o_proj + seq)\n",
    "        if not output_attentions:\n",
    "            return new_embedding\n",
    "        else:\n",
    "            return new_embedding, scaled_scores\n",
    "\n",
    "    def multihead_view(self, proj: torch.Tensor, transpose=False):\n",
    "        proj_view = proj.view(proj.shape[0], proj.shape[1], self.heads, self.size_per_head)\n",
    "        if not transpose:\n",
    "            return proj_view.permute(0, 2, 1, 3)\n",
    "        else:\n",
    "            return proj_view.permute(0, 2, 3, 1)\n",
    "\n",
    "\n",
    "class BertEncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, config: BertConfig):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.intermediate = config.inter_size\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.intermediate_proj = nn.Linear(in_features=self.d_model, out_features=self.intermediate)\n",
    "        self.intermediate_act = getattr(nn, config.inter_activation)()\n",
    "        self.out_proj = nn.Linear(in_features=self.intermediate, out_features=self.d_model)\n",
    "        self.out_dropout = nn.Dropout(config.hidden_dropout)\n",
    "        self.out_norm = nn.LayerNorm(self.d_model, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, seq: torch.Tensor, mask: torch.Tensor):\n",
    "        attn_out = self.attention(seq, mask)\n",
    "        inter_out = self.intermediate_act(self.intermediate_proj(attn_out))\n",
    "        layer_out = self.out_dropout(self.out_proj(inter_out))\n",
    "        return self.out_norm(layer_out + attn_out)\n",
    "\n",
    "\n",
    "def load_multihead_attention(tf_chk: str, layer: int, attention: MultiHeadAttention):\n",
    "    with torch.no_grad():\n",
    "        # K\n",
    "        load_tf_var(tf_chk, f\"bert/encoder/layer_{layer}/attention/self/key/bias\", attention.k_proj.bias)\n",
    "        load_tf_var(tf_chk, f\"bert/encoder/layer_{layer}/attention/self/key/kernel\", attention.k_proj.weight,\n",
    "                    processor=lambda x: np.transpose(x))\n",
    "\n",
    "        # Q\n",
    "        load_tf_var(tf_chk, f\"bert/encoder/layer_{layer}/attention/self/query/bias\", attention.q_proj.bias)\n",
    "        load_tf_var(tf_chk, f\"bert/encoder/layer_{layer}/attention/self/query/kernel\", attention.q_proj.weight,\n",
    "                    processor=lambda x: np.transpose(x))\n",
    "\n",
    "        # Q\n",
    "        load_tf_var(tf_chk, f\"bert/encoder/layer_{layer}/attention/self/value/bias\", attention.v_proj.bias)\n",
    "        load_tf_var(tf_chk, f\"bert/encoder/layer_{layer}/attention/self/value/kernel\", attention.v_proj.weight,\n",
    "                    processor=lambda x: np.transpose(x))\n",
    "\n",
    "        # Output\n",
    "        load_tf_var(tf_chk, f\"bert/encoder/layer_{layer}/attention/output/dense/bias\", attention.o_proj.bias)\n",
    "        load_tf_var(tf_chk, f\"bert/encoder/layer_{layer}/attention/output/dense/kernel\", attention.o_proj.weight,\n",
    "                    processor=lambda x: np.transpose(x))\n",
    "        load_tf_var(tf_chk, f\"bert/encoder/layer_{layer}/attention/output/LayerNorm/beta\", attention.output_norm.bias)\n",
    "        load_tf_var(tf_chk, f\"bert/encoder/layer_{layer}/attention/output/LayerNorm/gamma\",\n",
    "                    attention.output_norm.weight)\n",
    "\n",
    "\n",
    "def load_bert_encoder(tf_chk: str, layer: int, encoder: BertEncoderLayer):\n",
    "    load_multihead_attention(tf_chk, layer, encoder.attention)\n",
    "    with torch.no_grad():\n",
    "        # Intermediate\n",
    "        load_tf_var(tf_chk, f\"bert/encoder/layer_{layer}/intermediate/dense/bias\", encoder.intermediate_proj.bias)\n",
    "        load_tf_var(tf_chk, f\"bert/encoder/layer_{layer}/intermediate/dense/kernel\", encoder.intermediate_proj.weight,\n",
    "                    processor=lambda x: np.transpose(x))\n",
    "\n",
    "        # Output\n",
    "        load_tf_var(tf_chk, f\"bert/encoder/layer_{layer}/output/dense/bias\", encoder.out_proj.bias)\n",
    "        load_tf_var(tf_chk, f\"bert/encoder/layer_{layer}/output/dense/kernel\", encoder.out_proj.weight,\n",
    "                    processor=lambda x: np.transpose(x))\n",
    "        load_tf_var(tf_chk, f\"bert/encoder/layer_{layer}/output/LayerNorm/beta\", encoder.out_norm.bias)\n",
    "        load_tf_var(tf_chk, f\"bert/encoder/layer_{layer}/output/LayerNorm/gamma\", encoder.out_norm.weight)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.2949, -0.0026, -0.1349,  ...,  0.0461, -0.0124, -0.0396],\n        [ 0.4209,  0.6647,  0.4904,  ...,  0.4522, -0.9139, -0.2006],\n        [ 0.0837,  0.8036,  0.8864,  ...,  0.3865,  0.7934, -0.3622],\n        ...,\n        [ 1.1550, -0.6692,  0.2890,  ..., -0.7546, -0.9407, -0.4982],\n        [-0.0672,  0.2081,  0.3474,  ...,  0.3738,  0.4269,  0.0777],\n        [-0.1206, -0.1941,  0.1503,  ...,  0.6953, -0.6948,  0.4040]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_0 = BertEncoderLayer(config)\n",
    "load_bert_encoder(tf_chkpt, 0, layer_0)\n",
    "layer_0.eval()\n",
    "with torch.no_grad():\n",
    "    example_layer0 = layer_0(example, torch.IntTensor([exampled_tokenized[\"attention_mask\"]]))\n",
    "example_layer0[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class BertEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, config: BertConfig):\n",
    "        super().__init__()\n",
    "        self.embeddings = BertEmbedding(config)\n",
    "        self.encoder_layers = nn.ModuleList([BertEncoderLayer(config) for _ in range(config.encoder_layers)])\n",
    "\n",
    "    def forward(self, seq: torch.Tensor, mask: torch.Tensor, seq_seg: torch.Tensor):\n",
    "        word_embeddings = self.embeddings(seq, seq_seg)\n",
    "        final_embeddings = word_embeddings\n",
    "        for layer in self.encoder_layers:\n",
    "            final_embeddings = layer(final_embeddings, mask)\n",
    "        return final_embeddings\n",
    "\n",
    "\n",
    "class FirstTokenPooler(nn.Module):\n",
    "\n",
    "    def __init__(self, config: BertConfig):\n",
    "        super().__init__()\n",
    "        self.pool_proj = nn.Linear(in_features=config.d_model, out_features=config.d_model)\n",
    "        self.pool_act = nn.Tanh()\n",
    "\n",
    "    def forward(self, seq: torch.Tensor):\n",
    "        return self.pool_act(self.pool_proj(seq[:, 0, :]))\n",
    "\n",
    "\n",
    "def convert_tf_bert(tf_check: str, encoder: BertEncoder):\n",
    "    load_embeddings(tf_check, encoder.embeddings)\n",
    "    for i, layer in enumerate(encoder.encoder_layers):\n",
    "        load_bert_encoder(tf_check, i, layer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0.3731, -0.0692, -0.0403,  ..., -0.0555,  0.2904,  0.1068],\n         [ 0.3540, -0.4470,  0.6067,  ..., -0.2113,  0.0843, -0.0635],\n         [ 0.2962, -0.2089,  0.4200,  ...,  0.2340, -0.0422, -0.1688],\n         ...,\n         [ 0.4827, -0.2090,  0.2774,  ...,  0.0042,  0.2757,  0.1399],\n         [ 0.1953, -0.2299, -0.1022,  ...,  0.1420,  0.1598,  0.0443],\n         [ 0.1453, -0.0737, -0.1196,  ..., -0.2397,  0.6364, -0.0327]]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = BertConfig(vocab_size=28996)\n",
    "bert_encoder = BertEncoder(config)\n",
    "convert_tf_bert(tf_chkpt, bert_encoder)\n",
    "bert_encoder.eval()\n",
    "with torch.no_grad():\n",
    "    example = bert_encoder(torch.IntTensor([exampled_tokenized[\"input_ids\"]]),\n",
    "                           seq_seg=torch.IntTensor([exampled_tokenized[\"token_type_ids\"]]),\n",
    "                           mask=torch.IntTensor([exampled_tokenized[\"attention_mask\"]]))\n",
    "example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class BertMLMHead(nn.Module):\n",
    "\n",
    "    def __init__(self, config: BertConfig, word_embedding: nn.Embedding):\n",
    "        super().__init__()\n",
    "        self.transform = nn.Linear(in_features=config.d_model, out_features=config.d_model)\n",
    "        self.transform_act = getattr(nn, config.inter_activation)()\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=config.d_model)\n",
    "        self.word_embedding = word_embedding\n",
    "        self.bias = nn.Parameter(\n",
    "            data=torch.rand(config.vocab_size) * 2 * np.sqrt(config.d_model) - np.sqrt(config.d_model),\n",
    "            requires_grad=True)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        transformed_embeddings = self.layer_norm(self.transform_act(self.transform(seq)))\n",
    "        scores = torch.matmul(transformed_embeddings, self.word_embedding.weight.transpose(0, 1))\n",
    "        return scores + self.bias\n",
    "\n",
    "\n",
    "def load_mlm_head(tf_check: str, head: BertMLMHead):\n",
    "    with torch.no_grad():\n",
    "        # Transform\n",
    "        load_tf_var(tf_check, \"cls/predictions/transform/LayerNorm/beta\", head.layer_norm.bias)\n",
    "        load_tf_var(tf_check, \"cls/predictions/transform/LayerNorm/gamma\", head.layer_norm.weight)\n",
    "        load_tf_var(tf_check, \"cls/predictions/transform/dense/bias\", head.transform.bias)\n",
    "        load_tf_var(tf_check, \"cls/predictions/transform/dense/kernel\", head.transform.weight,\n",
    "                    processor=lambda x: np.transpose(x))\n",
    "\n",
    "        # Bias\n",
    "        load_tf_var(tf_check, \"cls/predictions/output_bias\", head.bias)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "BertMLMHead(\n  (transform): Linear(in_features=768, out_features=768, bias=True)\n  (transform_act): GELU(approximate='none')\n  (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (word_embedding): Embedding(28996, 768, padding_idx=0)\n)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_head = BertMLMHead(config, word_embedding=bert_encoder.embeddings.word_embedding)\n",
    "load_mlm_head(tf_chkpt, mlm_head)\n",
    "mlm_head.eval()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 119,  119, 1116, 1660, 3485, 1106, 1482,  119,  119]])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    example_representation = bert_encoder(seq=torch.IntTensor([exampled_tokenized[\"input_ids\"]]),\n",
    "                                          mask=torch.IntTensor([exampled_tokenized[\"attention_mask\"]]),\n",
    "                                          seq_seg=torch.IntTensor(exampled_tokenized[\"token_type_ids\"]))\n",
    "    scores = mlm_head(example_representation)\n",
    "prediction = torch.argmax(scores, dim=-1)\n",
    "prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "'birth'"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(prediction[0, masked])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading Transformers Weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": "[BertEmbeddings(\n   (word_embeddings): Embedding(28996, 768, padding_idx=0)\n   (position_embeddings): Embedding(512, 768)\n   (token_type_embeddings): Embedding(2, 768)\n   (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n   (dropout): Dropout(p=0.1, inplace=False)\n ),\n BertEncoder(\n   (layer): ModuleList(\n     (0-11): 12 x BertLayer(\n       (attention): BertAttention(\n         (self): BertSelfAttention(\n           (query): Linear(in_features=768, out_features=768, bias=True)\n           (key): Linear(in_features=768, out_features=768, bias=True)\n           (value): Linear(in_features=768, out_features=768, bias=True)\n           (dropout): Dropout(p=0.1, inplace=False)\n         )\n         (output): BertSelfOutput(\n           (dense): Linear(in_features=768, out_features=768, bias=True)\n           (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n           (dropout): Dropout(p=0.1, inplace=False)\n         )\n       )\n       (intermediate): BertIntermediate(\n         (dense): Linear(in_features=768, out_features=3072, bias=True)\n         (intermediate_act_fn): GELUActivation()\n       )\n       (output): BertOutput(\n         (dense): Linear(in_features=3072, out_features=768, bias=True)\n         (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n         (dropout): Dropout(p=0.1, inplace=False)\n       )\n     )\n   )\n )]"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers_bert = BertForMaskedLM.from_pretrained(\"bert-base-cased\")\n",
    "list(transformers_bert.base_model.children())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def load_transformers_embeddings(tf_embedding: BertEmbeddings, embeddings: BertEmbedding):\n",
    "    with torch.no_grad():\n",
    "        embeddings.word_embedding.weight.copy_(tf_embedding.word_embeddings.weight)\n",
    "        embeddings.pos_embedding.weight.copy_(tf_embedding.position_embeddings.weight)\n",
    "        embeddings.segment_embedding.weight.copy_(tf_embedding.token_type_embeddings.weight)\n",
    "\n",
    "\n",
    "def load_transformers_encoders(tf_layers: ModuleList, layers: List[BertEncoderLayer]):\n",
    "    with torch.no_grad():\n",
    "        for tf_l, l in zip(tf_layers, layers):\n",
    "            # Linear Weights\n",
    "            l.attention.k_proj.weight.copy_(tf_l.attention.self.key.weight)\n",
    "            l.attention.q_proj.weight.copy_(tf_l.attention.self.query.weight)\n",
    "            l.attention.v_proj.weight.copy_(tf_l.attention.self.value.weight)\n",
    "            l.attention.o_proj.weight.copy_(tf_l.attention.output.dense.weight)\n",
    "\n",
    "            # Linear Bias\n",
    "            l.attention.k_proj.bias.copy_(tf_l.attention.self.key.bias)\n",
    "            l.attention.q_proj.bias.copy_(tf_l.attention.self.query.bias)\n",
    "            l.attention.v_proj.bias.copy_(tf_l.attention.self.value.bias)\n",
    "            l.attention.o_proj.bias.copy_(tf_l.attention.output.dense.bias)\n",
    "\n",
    "            # Attention Norm\n",
    "            l.attention.output_norm.weight.copy_(tf_l.attention.output.LayerNorm.weight)\n",
    "            l.attention.output_norm.bias.copy_(tf_l.attention.output.LayerNorm.bias)\n",
    "\n",
    "            # Intermediate Linear\n",
    "            l.intermediate_proj.weight.copy_(tf_l.intermediate.dense.weight)\n",
    "            l.intermediate_proj.bias.copy_(tf_l.intermediate.dense.bias)\n",
    "\n",
    "            # Output Linear + Norm\n",
    "            l.out_proj.weight.copy_(tf_l.output.dense.weight)\n",
    "            l.out_proj.bias.copy_(tf_l.output.dense.bias)\n",
    "            l.out_norm.weight.copy_(tf_l.output.LayerNorm.weight)\n",
    "            l.out_norm.bias.copy_(tf_l.output.LayerNorm.bias)\n",
    "\n",
    "\n",
    "def load_transformers_base_bert(tf_bert: BertModel, bert_base: BertEncoder):\n",
    "    load_transformers_embeddings(tf_bert.embeddings, bert_base.embeddings)\n",
    "    load_transformers_encoders(tf_bert.encoder.layer, bert_base.encoder_layers)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0.3731, -0.0692, -0.0403,  ..., -0.0555,  0.2904,  0.1068],\n         [ 0.3540, -0.4470,  0.6067,  ..., -0.2113,  0.0843, -0.0635],\n         [ 0.2962, -0.2089,  0.4200,  ...,  0.2340, -0.0422, -0.1688],\n         ...,\n         [ 0.4827, -0.2090,  0.2774,  ...,  0.0042,  0.2757,  0.1399],\n         [ 0.1953, -0.2299, -0.1022,  ...,  0.1420,  0.1598,  0.0443],\n         [ 0.1453, -0.0737, -0.1196,  ..., -0.2397,  0.6364, -0.0327]]])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_transformers_base_bert(transformers_bert.base_model, bert_encoder)\n",
    "bert_encoder.eval()\n",
    "with torch.no_grad():\n",
    "    example = bert_encoder(torch.IntTensor([exampled_tokenized[\"input_ids\"]]),\n",
    "                           seq_seg=torch.IntTensor([exampled_tokenized[\"token_type_ids\"]]),\n",
    "                           mask=torch.IntTensor([exampled_tokenized[\"attention_mask\"]]))\n",
    "example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0.3731, -0.0692, -0.0403,  ..., -0.0555,  0.2904,  0.1068],\n         [ 0.3540, -0.4470,  0.6067,  ..., -0.2113,  0.0843, -0.0635],\n         [ 0.2962, -0.2089,  0.4200,  ...,  0.2340, -0.0422, -0.1688],\n         ...,\n         [ 0.4827, -0.2090,  0.2774,  ...,  0.0042,  0.2757,  0.1399],\n         [ 0.1953, -0.2299, -0.1022,  ...,  0.1420,  0.1598,  0.0443],\n         [ 0.1453, -0.0737, -0.1196,  ..., -0.2397,  0.6364, -0.0327]]],\n       grad_fn=<NativeLayerNormBackward0>)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_example = transformers_bert.base_model(input_ids=torch.IntTensor([exampled_tokenized[\"input_ids\"]]),\n",
    "                                          attention_mask=torch.IntTensor([exampled_tokenized[\"attention_mask\"]]),\n",
    "                                          token_type_ids=torch.IntTensor(\n",
    "                                              exampled_tokenized[\"token_type_ids\"])).last_hidden_state\n",
    "tf_example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(1.4656e-07, grad_fn=<MeanBackward0>)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.abs(example - tf_example))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.abs(example_representation - example))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 119,  119, 1116, 1660, 3485, 1106, 1482,  119,  119]])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_mlm = transformers_bert(input_ids=torch.IntTensor([exampled_tokenized[\"input_ids\"]]),\n",
    "                                          attention_mask=torch.IntTensor([exampled_tokenized[\"attention_mask\"]]),\n",
    "                                          token_type_ids=torch.IntTensor(\n",
    "                                              exampled_tokenized[\"token_type_ids\"]))\n",
    "tf_prediction = torch.argmax(tf_mlm.logits, dim=-1)\n",
    "tf_prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 119,  119, 1116, 1660, 3485, 1106, 1482,  119,  119]])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
