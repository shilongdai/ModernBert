{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 17:36:16.772880: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-17 17:36:16.772913: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-17 17:36:16.773871: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-17 17:36:16.778781: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-17 17:36:17.345546: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "random.seed(93)\n",
    "np.random.seed(93)\n",
    "torch.manual_seed(93)\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "[('bert/embeddings/LayerNorm/beta', [768]),\n ('bert/embeddings/LayerNorm/gamma', [768]),\n ('bert/embeddings/position_embeddings', [512, 768]),\n ('bert/embeddings/token_type_embeddings', [2, 768]),\n ('bert/embeddings/word_embeddings', [28996, 768]),\n ('bert/encoder/layer_0/attention/output/LayerNorm/beta', [768]),\n ('bert/encoder/layer_0/attention/output/LayerNorm/gamma', [768]),\n ('bert/encoder/layer_0/attention/output/dense/bias', [768]),\n ('bert/encoder/layer_0/attention/output/dense/kernel', [768, 768]),\n ('bert/encoder/layer_0/attention/self/key/bias', [768]),\n ('bert/encoder/layer_0/attention/self/key/kernel', [768, 768]),\n ('bert/encoder/layer_0/attention/self/query/bias', [768]),\n ('bert/encoder/layer_0/attention/self/query/kernel', [768, 768]),\n ('bert/encoder/layer_0/attention/self/value/bias', [768]),\n ('bert/encoder/layer_0/attention/self/value/kernel', [768, 768]),\n ('bert/encoder/layer_0/intermediate/dense/bias', [3072]),\n ('bert/encoder/layer_0/intermediate/dense/kernel', [768, 3072]),\n ('bert/encoder/layer_0/output/LayerNorm/beta', [768]),\n ('bert/encoder/layer_0/output/LayerNorm/gamma', [768]),\n ('bert/encoder/layer_0/output/dense/bias', [768]),\n ('bert/encoder/layer_0/output/dense/kernel', [3072, 768])]"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_chkpt = \"./models/cased_L-12_H-768_A-12/bert_model.ckpt\"\n",
    "tf.train.list_variables(tf_chkpt)[:21]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class BertConfig:\n",
    "\n",
    "    def __init__(self, vocab_size: int, vocab_pad: int = 0, d_model: int = 768,\n",
    "                 seq_len: int = 512, attention_heads = 12, layer_norm_eps=1e-5):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab_pad = vocab_pad\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.attention_head = attention_heads\n",
    "\n",
    "\n",
    "class BertEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, config: BertConfig):\n",
    "        super().__init__()\n",
    "        self.word_embedding = nn.Embedding(num_embeddings=config.vocab_size, embedding_dim=config.d_model, padding_idx=config.vocab_pad)\n",
    "        self.segment_embedding = nn.Embedding(num_embeddings=3, embedding_dim=config.d_model, padding_idx=2)\n",
    "        self.pos_embedding = nn.Embedding(num_embeddings=config.seq_len, embedding_dim=config.d_model)\n",
    "        self.layer_norm = nn.LayerNorm(config.d_model, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, seq, seq_seg):\n",
    "        embedding = self.word_embedding(seq) + self.pos_embedding(seq) + self.segment_embedding(seq_seg)\n",
    "        return self.dropout(embedding)\n",
    "\n",
    "\n",
    "def load_tf_var(chpt: str, src_var: str, target: nn.parameter.Parameter, processor=lambda x: x):\n",
    "    src_val = tf.train.load_variable(chpt, src_var)\n",
    "    src_val = processor(src_val)\n",
    "    src_val = torch.from_numpy(src_val).float()\n",
    "    target.copy_(src_val)\n",
    "\n",
    "\n",
    "def load_embeddings(tf_chk, embedding: BertEmbedding):\n",
    "    with torch.no_grad():\n",
    "        # layer norm\n",
    "        load_tf_var(tf_chk, \"bert/embeddings/LayerNorm/gamma\", embedding.layer_norm.weight)\n",
    "        load_tf_var(tf_chk, \"bert/embeddings/LayerNorm/beta\", embedding.layer_norm.bias)\n",
    "\n",
    "        # token embeddings\n",
    "        embedding_shape = embedding.word_embedding.weight.shape\n",
    "        load_tf_var(tf_chk, \"bert/embeddings/word_embeddings\", embedding.word_embedding.weight)\n",
    "        load_tf_var(tf_chk, \"bert/embeddings/position_embeddings\", embedding.pos_embedding.weight)\n",
    "        load_tf_var(tf_chk, \"bert/embeddings/token_type_embeddings\", embedding.segment_embedding.weight,\n",
    "                    lambda matrix: np.vstack([matrix, np.zeros(embedding_shape[1])]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-0.0139, -0.0442, -0.0051,  ..., -0.0000, -0.0369, -0.0127],\n         [-0.0161, -0.0390,  0.0132,  ..., -0.0043, -0.0360, -0.0300]]])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = BertConfig(vocab_size=28996)\n",
    "embedding = BertEmbedding(config)\n",
    "load_embeddings(tf_chkpt, embedding)\n",
    "with torch.no_grad():\n",
    "    example = embedding(torch.IntTensor([[1, 2]]), seq_seg=torch.IntTensor([[1, 2]]))\n",
    "example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-0.0139, -0.0442, -0.0051,  ..., -0.0126, -0.0369, -0.0127],\n         [-0.0189, -0.0433,  0.0095,  ..., -0.0070, -0.0384, -0.0326]]])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    example = embedding(torch.IntTensor([[1, 2]]), seq_seg=torch.IntTensor([[1, 1]]))\n",
    "example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config: BertConfig):\n",
    "        super().__init__()\n",
    "        assert config.d_model % config.attention_head == 0\n",
    "\n",
    "        self.heads = config.attention_head\n",
    "        self.size_per_head = config.d_model // self.heads\n",
    "        self.d_model = config.d_model\n",
    "\n",
    "        self.q_proj = nn.Linear(in_features=self.d_modelm, out_features=self.d_model)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
